\documentclass{beamer}

\mode<presentation>{
\usetheme{Madrid}
%\usecolortheme{beaver}
}
\usepackage[utf8]{inputenc}
\usepackage{default}
\usepackage{polski} %{babel}
\usepackage{pgfplots}
\pgfplotsset{/pgf/number format/use comma,compat=newest}
\usepackage{color}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{listings}

%\usebackgroundtemplate{%
%\tikz\node[opacity=0.05] {\includegraphics[height=\paperheight,width=\paperwidth]{logoh_2.png}};}


\title[Data mining and analysis]{Data mining and analysis}
\author[Ciejka, P. G.]{Paweł G. Ciejka}
%\institute[UCS]{Universidade de Caxias do Sul - Brasil}
\date{\today}

\begin{document}

\begin{frame}
 \maketitle
\end{frame}

\begin{frame}
\frametitle{Table of contents}
 \tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
        Data analysis consists of three layers, that are respectively gathering, filtering and searching.
        Gathering layer involves obtaining data from various sources that are in this case mainly websites and emails. 
    \end{minipage}
\end{frame} 




\section{}
\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
        Measuring the similarity between words, sentences,
        paragraphs and documents is an important component in
        various tasks such as information retrieval, document
        clustering, word-sense disambiguation, automatic essay
        scoring, short answer grading, machine translation and text
        summarization. This survey discusses the existing works on
        text similarity through partitioning them into three
        approaches; String-based, Corpus-based and Knowledgebased
        similarities. Furthermore, samples of combination
        between these similarities are presented.
    \end{minipage}
\end{frame}    

\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
        Text similarity measures play an increasingly important role
        in text related research and applications in tasks such as
        information retrieval, text classification, document clustering,
        topic detection, topic tracking, questions generation, question
        answering, essay scoring, short answer scoring, machine
        translation, text summarization and others. Finding similarity
        between words is a fundamental part of text similarity which
        is then used as a primary stage for sentence, paragraph and
        document similarities.      
    \end{minipage}
\end{frame}        
 
\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth} 
        Words can be similar in two ways lexically and semantically. Words are similar lexically if they
        have a similar character sequence. Words are similar
        semantically if they have the same thing, are opposite of each
        other, used in the same way, used in the same context and one
        is a type of another. Lexical similarity is introduced in this
        survey though different String-Based algorithms, Semantic
        similarity is introduced through Corpus-Based and
        Knowledge-Based algorithms. String-Based measures operate
        on string sequences and character composition. A string
        metric is a metric that measures similarity or dissimilarity
        (distance) between two text strings for approximate string
        matching or comparison. Corpus-Based similarity is a
        semantic similarity measure that determines the similarity
        between words according to information gained from large
        corpora. Knowledge-Based similarity is a semantic similarity
        measure that determines the degree of similarity between
        words using information derived from semantic networks. 
    \end{minipage}
\end{frame}    

\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
        The World Wide Web and its associated distributed information services, such as
        Yahoo!, Google, America Online, and AltaVista, provide rich, worldwide, on-line information
        services, where data objects are linked together to facilitate interactive access.
        Users seeking information of interest traverse from one object via links to another.
        Such systems provide ample opportunities and challenges for data mining. For example,
        understanding user access patterns will not only help improve system design (by
        providing efficient access between highly correlated objects), but also leads to better
        marketing decisions (e.g., by placing advertisements in frequently visited documents,
        or by providing better customer/user classification and behavior analysis). Capturing
        user access patterns in such distributed information environments is called Web usage
        mining (or Weblog mining).
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}        
        Although Web pages may appear fancy and informative to human readers, they can be
        highly unstructured and lack a predefined schema, type, or pattern. Thus it is difficult for
        computers to understand the semantic meaning of diverse Web pages and structure them
        in an organized way for systematic information retrieval and data mining. Web services
        that provide keyword-based searches without understanding the context behind the Web
        pages can only offer limited help to users. For example, a Web search based on a single
        keyword may return hundreds of Web page pointers containing the keyword, but most
        of the pointers will be very weakly related to what the user wants to find. Data mining
        can often provide additional help here than Web search services. For example, authoritative
        Web page analysis based on linkages among Web pages can help rank Web pages based on their importance, influence, and topics. Automated Web page clustering and
        classification help group and arrange Web pages in a multidimensional manner based
        on their contents. Web community analysis helps identify hidden Web social networks
        and communities and observe their evolution. Web mining is the development of scalable
        and effective Web data analysis and mining methods. It may help us learn about the
        distribution of information on the Web in general, characterize and classify Web pages,
        and uncover Web dynamics and the association and other relationships among different
        Web pages, users, communities, and Web-based activities.
    \end{minipage}
\end{frame}


\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
        Prolog is a general-purpose logic programming language associated with artificial intelligence and computational linguistics.
        Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is declarative: the program logic is expressed in terms of relations, represented as facts and rules. A computation is initiated by running a query over these relations.
        Prolog was one of the first logic programming languages, and remains the most popular among such languages today, with several free and commercial implementations available. The language has been used for theorem proving, expert systems, term rewriting, type inference, and automated planning, as well as its original intended field of use, natural language processing. Modern Prolog environments support the creation of graphical user interfaces, as well as administrative and networked applications.
        Prolog is well-suited for specific tasks that benefit from rule-based logical queries such as searching databases, voice control systems, and filling templates.
    \end{minipage}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
\begin{lstlisting}
    mother_child(trude, sally).
        
        father_child(tom, sally).
        father_child(tom, erica).
        father_child(mike, tom).
        
        sibling(X, Y)      :- parent_child(Z, X), parent_child(Z, Y).
        
        parent_child(X, Y) :- father_child(X, Y).
        parent_child(X, Y) :- mother_child(X, Y).

        ?- sibling(sally, erica).
        Yes    
\end{lstlisting}
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
        A regular expression, regex or regexp (sometimes called a rational expression) is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern. Usually this pattern is then used by string searching algorithms for "find" or "find and replace" operations on strings.

        The concept arose in the 1950s when the American mathematician Stephen Cole Kleene formalized the description of a regular language. The concept came into common use with Unix text-processing utilities. Today, different syntaxes for writing regular expressions exist, one being the POSIX standard and another, widely used, being the Perl syntax.

        Regular expressions are used in search engines, search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK and in lexical analysis. Many programming languages provide regex capabilities, built-in, or via libraries.
    \end{minipage}
\end{frame}
        
\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
        In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.        
\end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
        Parsing, syntax analysis or syntactic analysis is the process of analysing a string of symbols, either in natural language or in computer languages, conforming to the rules of a formal grammar. 
        The term has slightly different meanings in different branches of linguistics and computer science. Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams. It usually emphasizes the importance of grammatical divisions such as subject and predicate.

        Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information.
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
        In computer science, Backus–Naur form or Backus normal form (BNF) is a notation technique for context-free grammars, often used to describe the syntax of languages used in computing, such as computer programming languages, document formats, instruction sets and communication protocols. They are applied wherever exact descriptions of languages are needed: for instance, in official language specifications, in manuals, and in textbooks on programming language theory.
        Many extensions and variants of the original Backus–Naur notation are used; some are exactly defined, including extended Backus–Naur form (EBNF) and augmented Backus–Naur form (ABNF).
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
       % \begin{figure}[H]
       %     \includegraphics[scale=0.5]{Architecture.png}
       % \end{figure}
    \end{minipage}
\end{frame}  

\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
       % \begin{figure}[H]
       %     \includegraphics[scale=0.5]{parse.jpg}
       % \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
       % \begin{figure}[H]
       %     \includegraphics[scale=0.5]{parse2.png}
       % \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Introduction}
    \begin{minipage}{\textwidth}
       % \begin{figure}[H]
        %    \includegraphics[scale=0.5]{ast_green_dot.jpg}
        %\end{figure}
    \end{minipage}
\end{frame}

% \section{Tabela}
% \begin{frame}
%  \frametitle{Tabela}
% \begin{minipage}{\textwidth}
% \begin{table}[ht]
%   \caption{Resultados teóricos em função de $\lambda$.}
%   \begin{tabular}{c|c|c|c}
%   \hline\hline 
%   $\lambda~(nm)$&$h~(eV.s)$&$c~(m/s)$&$E~(eV)$ \\
%   \hline 
%   238&$4,136~x~10^{-15}$&$3~x~10^{8}$&5,21 \\
%   532&$4,136~x~10^{-15}$&$3~x~10^{8}$&2,33 \\
%   680&$4,136~x~10^{-15}$&$3~x~10^{8}$&1,82 \\
%   \hline\hline
%  \end{tabular}
% \end{table}
% 
% \end{minipage}
% \end{frame}
% 
% \section{Figuras}
% \begin{frame}
%  \frametitle{Gráfico}
% \begin{center} 
% \begin{tikzpicture}
% \begin{axis}
% [xlabel=$x$,
% ylabel={$f(x)=x^2-x+4$}, grid=major]
% \addplot {x^2-x+4};
% \legend{$d=2$}
% \end{axis}
% \end{tikzpicture}
% \end{center}
% \end{frame}
% 
% \section{Referências}
% \begin{frame}
%  \frametitle{Referências}
%  \begin{thebibliography}{5}
%   \bibitem{beamer} \emph{Tantau T., Wright J. and Mileti\'{c} V. The BEAMER class - User Guide for version 3.36
%   , March 2015}.
%   \bibitem{beamer2} \emph{Mertz A. and Slough W. Beamer by Example. The Prac\TeX~ Journal, n. 4, 2005.}
%  \end{thebibliography}
% 
% \end{frame}

\end{document}
 
